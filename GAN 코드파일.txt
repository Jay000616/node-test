import os
os.environ[ "KMP_DUPLICATE_LIB_OK" ]="TRUE"
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms, utils
import torchvision.utils as vutils
import matplotlib.pyplot as plt
import numpy as np

class Generator(nn.Module):
     def __init__(self):
             super(Generator, self).__init__()
             self.main = nn.Sequential(
                     nn.ConvTranspose2d(100, 512, 4, 1 , 0, bias=False),
                     nn.BatchNorm2d(512),
                     nn.ReLU(True),
                     nn.ConvTranspose2d(512, 256, 4, 2 , 1, bias=False),
                     nn.BatchNorm2d(256),
                     nn.ReLU(True),
                     nn.ConvTranspose2d(256, 128, 4, 2 , 1, bias=False),
                     nn.BatchNorm2d(128),
                     nn.ReLU(True),
                     nn.ConvTranspose2d(128, 64, 4, 2 , 1, bias=False),
                     nn.BatchNorm2d(64),
                     nn.ReLU(True),
                     nn.ConvTranspose2d(64, 3, 4, 2 , 1, bias=False),
                     nn.Tanh()
             )
     def forward(self, input):
             return self.main(input)

class Discriminator(nn.Module):
     def __init__(self):
             super(Discriminator, self).__init__()
             self.main = nn.Sequential(
                     nn.Conv2d(3, 64, 4, 2 , 1, bias=False),
                     nn.LeakyReLU(0.2, inplace=True),
                     nn.Conv2d(64, 128, 4, 2 , 1, bias=False),
                     nn.BatchNorm2d(128),
                     nn.LeakyReLU(0.2, inplace=True),
                     nn.Conv2d(128, 256, 4, 2 , 1, bias=False),
                     nn.BatchNorm2d(256),
                     nn.LeakyReLU(0.2, inplace=True),
                     nn.Conv2d(256, 512, 4, 2 , 1, bias=False),
                     nn.BatchNorm2d(512),
                     nn.LeakyReLU(0.2, inplace=True),
                     nn.Conv2d(512, 1, 4, 1, 0, bias=False),
                     nn.Sigmoid()
             )
     def forward(self, input):
             return self.main(input).view(-1, 1).squeeze(1)

def weights_init(m):
     classname = m.__class__.__name__
     if classname.find('Conv') !=-1:
             nn.init.normal_(m.weight.data, 0.0, 0.02)
     elif classname.find('BatchNorm') != -1:
             nn.init.normal_(m.weight.data, 1.0, 0.02)
             nn.init.constant_(m.bias.data, 0)

def show_generated_images(images, num_images=64):
     plt.figure(figsize=(10, 10))
     plt.axis("off")
     plt.title("Generated Images")
     images = vutils.make_grid(images[:num_images], padding=2, normalize=True)
     images = np.transpose(images.cpu(), (1,2,0))
     plt.imshow(images)
     plt.show()

def save_generated_images(images, num_images, epoch, idx):
     plt.figure(figsize=(10, 10))
     plt.axis("off")
     plt.title("Generated Images")
     images = vutils.make__gird(images[:num_images], padding=2, normalize=True)
     images = np.transpose(images.cpu(), (1,2,0))
     fname = './output/image_'+str(epoch)+'_'+str(idx)+'.jpg'
     plt.imsave(fname, images.numpy())

netG = Generator()
netD = Discriminator()

netG.apply(weights_init)
netD.apply(weights_init)

transform = transforms.Compose([
	transforms.Resize(64),
	transforms.CenterCrop(64),
	transforms.ToTensor(),
	transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),
])

dataset = datasets.ImageFolder(root= './data/celeba', transform=transform)
dataloader = DataLoader(dataset, batch_size=128, shuffle=True)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
netG.to(device)
netD.to(device)

criterion = nn.BCELoss()
optimizerD = optim.Adam(netD.parameters(), Ir=0.002, betas=(0.5, 0.999))
optimizerG = optim.Adam(netG.parameters(), Ir=0.002, betas=(0.5, 0.999))

num_epochs = 10

fixed_noise = torch.radn(64, 100, 1, 1, device=device)

for epoch in range(num_epochs):
	for i, data in enumerate(dataloader, 0):
		netD.zero_grad()
		real_data = data[0].to(device)
		batch_size = real_data.size(0)
		real_label = torch.full((batch_size,), 1, dtype=torch.float, device=device)
		fake_label = torch.full((batch_size,), 0, dtype=torch.float, device=device)

		output = netD(real_data).view(-1)
		errD_real = criterion(output, real_label)
		errD_real.backward()

		noise = torch.randn(batch_size, 100, 1,1, device=device)
		fake_data = netG(noise)
		output = netD(fake_data.detach()).view(-1)
		errD_fake = criterion(output, fake_label)
		errD_fake.backward()

		errD = errD_real + errD_fake
		optimizerD.step()

		netG.zero_grad()
		output = netD(fake_data).view(-1)
		errG = criterion(output, real_label)
		errG.backward()
		optimizerG.step()

		if i % 50 == 0:
			print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f'
				% (epoch, num_epochs, i, Len(dataloader), errD.item(), errG.item()))
			fake_images = netG(fixed_noise)
			save_generated_images(fake_images, 64, epoch=epoch, idx=i)

fake_images = netG(fixed_noise)
show_generated_images(fake_images)
	


























...             nn.init.constant_(m.bias.data, 0)